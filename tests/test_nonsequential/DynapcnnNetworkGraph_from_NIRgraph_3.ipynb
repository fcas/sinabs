{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetworkGraph\n",
    "from sinabs.layers import Merge, IAFSqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f426a2d4c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "input_shape = (channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Module (Model #1)\n",
    "\n",
    "This one won't pass the `config_builder.validate_configuration` because the nodes inputing into the `Merge` layer don't have the same output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5, 1, bias=False)\n",
    "        self.iaf1 = IAFSqueeze(batch_size=1)\n",
    "        self.pool1 = nn.AvgPool2d(2,2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5, 1, bias=False)\n",
    "        self.iaf2 = IAFSqueeze(batch_size=1)\n",
    "        self.pool2 = nn.AvgPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(20, 1, 3, 1, bias=False)\n",
    "        self.iaf3 = IAFSqueeze(batch_size=1)\n",
    "        self.pool3 = nn.AvgPool2d(2,2)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(121, 500, bias=False)\n",
    "        self.iaf4 = IAFSqueeze(batch_size=1)\n",
    "        self.fc2 = nn.Linear(500, 10, bias=False)\n",
    "        self.iaf5 = IAFSqueeze(batch_size=1)\n",
    "\n",
    "        self.adder = Merge()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        con1_out = self.conv1(x)\n",
    "        iaf1_out = self.iaf1(con1_out)\n",
    "        pool1_out = self.pool1(iaf1_out)\n",
    "\n",
    "        conv2_out = self.conv2(pool1_out)\n",
    "        iaf2_out = self.iaf2(conv2_out)\n",
    "        pool2_out = self.pool2(iaf2_out)\n",
    "\n",
    "        conv3_out = self.conv3(self.adder(iaf1_out, pool2_out))\n",
    "        iaf3_out = self.iaf3(conv3_out)\n",
    "        pool3_out = self.pool3(iaf3_out)\n",
    "\n",
    "        flat_out = self.flat(pool3_out)\n",
    "        \n",
    "        fc1_out = self.fc1(flat_out)\n",
    "        iaf4_out = self.iaf4(fc1_out)\n",
    "        fc2_out = self.fc2(iaf4_out)\n",
    "        iaf5_out = self.iaf5(fc2_out)\n",
    "\n",
    "        return iaf5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = SNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, *input_shape))\n",
    "\n",
    "con1_out = snn.conv1(x)\n",
    "iaf1_out = snn.iaf1(con1_out)\n",
    "print('iaf1_out: ', iaf1_out.shape)\n",
    "pool1_out = snn.pool1(iaf1_out)\n",
    "\n",
    "conv2_out = snn.conv2(pool1_out)\n",
    "iaf2_out = snn.iaf2(conv2_out)\n",
    "pool2_out = snn.pool2(iaf2_out)\n",
    "print('pool2_out: ', pool2_out.shape)\n",
    "\n",
    "added = snn.adder(iaf1_out, pool2_out)\n",
    "print('added: ', added.shape)\n",
    "\n",
    "conv3_out = snn.conv3(added)\n",
    "iaf3_out = snn.iaf3(conv3_out)\n",
    "pool3_out = snn.pool3(iaf3_out)\n",
    "print('pool3_out: ', pool3_out.shape)\n",
    "\n",
    "flat_out = snn.flat(pool3_out)\n",
    "print('flat_out: ', flat_out.shape)\n",
    "\n",
    "fc1_out = snn.fc1(flat_out)\n",
    "iaf4_out = snn.iaf4(fc1_out)\n",
    "fc2_out = snn.fc2(iaf4_out)\n",
    "iaf5_out = snn.iaf5(fc2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'DynapcnnLayer 0 [core 0]:')\n",
    "print(f'                                                input: {x.shape}')\n",
    "con1_out = snn.conv1(x)\n",
    "print(f'                                                conv1: {con1_out.shape}')\n",
    "iaf1_out = snn.iaf1(con1_out)\n",
    "print(f'                                                iaf1: {iaf1_out.shape}')\n",
    "pool1_out = snn.pool1(iaf1_out)\n",
    "print(f'                                                pool1: {pool1_out.shape}\\n')\n",
    "\n",
    "print(f'DynapcnnLayer 1 [core 1]:')\n",
    "print(f'                                                input: {pool1_out.shape}')\n",
    "conv2_out = snn.conv2(pool1_out)\n",
    "print(f'                                                conv2: {conv2_out.shape}')\n",
    "iaf2_out = snn.iaf2(conv2_out)\n",
    "print(f'                                                iaf2: {iaf2_out.shape}')\n",
    "pool2_out = snn.pool2(iaf2_out)\n",
    "print(f'                                                pool2: {pool2_out.shape}\\n')\n",
    "\n",
    "added = snn.adder(iaf1_out, pool2_out)\n",
    "\n",
    "print(f'DynapcnnLayer 2 [core 2]:')\n",
    "print(f'                                                input: {added.shape}')\n",
    "conv3_out = snn.conv3(added)\n",
    "print(f'                                                conv3: {conv3_out.shape}')\n",
    "iaf3_out = snn.iaf3(conv3_out)\n",
    "print(f'                                                iaf3: {iaf3_out.shape}')\n",
    "pool3_out = snn.pool3(iaf3_out)\n",
    "print(f'                                                pool3: {pool3_out.shape}')\n",
    "\n",
    "flat_out = snn.flat(pool3_out)\n",
    "\n",
    "fc1_out = snn.fc1(flat_out)\n",
    "iaf4_out = snn.iaf4(fc1_out)\n",
    "print(f'DynapcnnLayer 3: {iaf4_out.shape}')\n",
    "\n",
    "fc2_out = snn.fc2(iaf4_out)\n",
    "iaf5_out = snn.iaf5(fc2_out)\n",
    "print(f'DynapcnnLayer 4: {iaf5_out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynapCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x625 and 16x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hw_model \u001b[38;5;241m=\u001b[39m DynapcnnNetworkGraph(\n\u001b[1;32m      2\u001b[0m     snn,\n\u001b[1;32m      3\u001b[0m     discretize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39minput_shape\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/github/sinabs/sinabs/backend/dynapcnn/dynapcnn_network_graph.py:71\u001b[0m, in \u001b[0;36mDynapcnnNetworkGraph.__init__\u001b[0;34m(self, snn, input_shape, dvs_input, discretize)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer_input_shape did not return 3-tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_tracer \u001b[38;5;241m=\u001b[39m NIRtoDynapcnnNetworkGraph(              \u001b[38;5;66;03m# computational graph from original PyTorch module.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     snn,\n\u001b[1;32m     73\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape)))                    \u001b[38;5;66;03m# needs the batch dimension.        \u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msinabs_edges, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msinabs_modules_map, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sinabs_edges_and_modules()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynapcnn_layers, \\\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes_to_dcnnl_map, \\\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdcnnl_to_dcnnl_map \u001b[38;5;241m=\u001b[39m build_from_graph(         \u001b[38;5;66;03m# build model from graph edges.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msinabs_edges,\n\u001b[1;32m     84\u001b[0m     merge_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_nodes)\n",
      "File \u001b[0;32m~/Documents/github/sinabs/sinabs/backend/dynapcnn/NIRGraphExtractor.py:10\u001b[0m, in \u001b[0;36mNIRtoDynapcnnNetworkGraph.__init__\u001b[0;34m(self, spiking_model, dummy_input)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spiking_model, dummy_input) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" .\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     nir_graph \u001b[38;5;241m=\u001b[39m nirtorch\u001b[38;5;241m.\u001b[39mextract_torch_graph(spiking_model, dummy_input, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mignore_tensors()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_2_indx_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_edges_from_nir(nir_graph)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_named_modules(spiking_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nirtorch/graph.py:433\u001b[0m, in \u001b[0;36mextract_torch_graph\u001b[0;34m(model, sample_data, model_name, model_args)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract computational graph between various modules in the model\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03mNOTE: This method is not capable of any compute happening outside of module\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mdefinitions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    Graph: A graph object representing the computational graph of the given model\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m GraphTracer(\n\u001b[1;32m    431\u001b[0m     named_modules_map(model, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m    432\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m tracer, torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 433\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model(sample_data, \u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# HACK: The current graph is using copy-constructors, that detaches\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# the traced output_types from the original graph.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# In the future, find a way to synchronize the two representations\u001b[39;00m\n\u001b[1;32m    438\u001b[0m tracer\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mmodule_output_types \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39moutput_types\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nirtorch/graph.py:344\u001b[0m, in \u001b[0;36mmodule_forward_wrapper.<locals>.my_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_forward\u001b[39m(mod: nn\u001b[38;5;241m.\u001b[39mModule, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 344\u001b[0m     out \u001b[38;5;241m=\u001b[39m _torch_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    347\u001b[0m         out_tuple \u001b[38;5;241m=\u001b[39m (out[\u001b[38;5;241m0\u001b[39m],)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m, in \u001b[0;36mSNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m iaf3_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miaf3(conv3_out)\n\u001b[1;32m     87\u001b[0m flat_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat(iaf3_out)\n\u001b[0;32m---> 89\u001b[0m fc1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(flat_out)\n\u001b[1;32m     90\u001b[0m iaf4_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miaf4(fc1_out)\n\u001b[1;32m     91\u001b[0m fc2_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(iaf4_out)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nirtorch/graph.py:344\u001b[0m, in \u001b[0;36mmodule_forward_wrapper.<locals>.my_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_forward\u001b[39m(mod: nn\u001b[38;5;241m.\u001b[39mModule, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 344\u001b[0m     out \u001b[38;5;241m=\u001b[39m _torch_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    347\u001b[0m         out_tuple \u001b[38;5;241m=\u001b[39m (out[\u001b[38;5;241m0\u001b[39m],)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x625 and 16x500)"
     ]
    }
   ],
   "source": [
    "hw_model = DynapcnnNetworkGraph(\n",
    "    snn,\n",
    "    discretize=True,\n",
    "    input_shape=input_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer[2] input size x: 24 different than Layer[0] output size x: 24 pooling: 2\n",
      "Layer[2] input size y: 24 different than Layer[0] output size y: 24 pooling: 2\n",
      "Layer[2] input size x: 24 different than Layer[1] output size x: 8 pooling: 2\n",
      "Layer[2] input size y: 24 different than Layer[1] output size y: 8 pooling: 2\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Generated config is not valid for speck2edevkit:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hw_model\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeck2edevkit:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/sinabs/sinabs/backend/dynapcnn/dynapcnn_network_graph.py:148\u001b[0m, in \u001b[0;36mDynapcnnNetworkGraph.to\u001b[0;34m(self, device, chip_layers_ordering, monitor_layers, config_modifier, slow_clk_frequency)\u001b[0m\n\u001b[1;32m    144\u001b[0m device_name, _ \u001b[38;5;241m=\u001b[39m parse_device_id(device)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_name \u001b[38;5;129;01min\u001b[39;00m ChipFactory\u001b[38;5;241m.\u001b[39msupported_devices:                \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_config(                                  \u001b[38;5;66;03m# generate config.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m         chip_layers_ordering\u001b[38;5;241m=\u001b[39mchip_layers_ordering,\n\u001b[1;32m    150\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    151\u001b[0m         monitor_layers\u001b[38;5;241m=\u001b[39mmonitor_layers,\n\u001b[1;32m    152\u001b[0m         config_modifier\u001b[38;5;241m=\u001b[39mconfig_modifier,\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamna_device \u001b[38;5;241m=\u001b[39m open_device(device)                     \u001b[38;5;66;03m# apply configuration to device.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamna_device\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mapply_configuration(config)\n",
      "File \u001b[0;32m~/Documents/github/sinabs/sinabs/backend/dynapcnn/dynapcnn_network_graph.py:256\u001b[0m, in \u001b[0;36mDynapcnnNetworkGraph.make_config\u001b[0;34m(self, chip_layers_ordering, device, monitor_layers, config_modifier)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated config is not valid for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Generated config is not valid for speck2edevkit:0"
     ]
    }
   ],
   "source": [
    "hw_model.to(device=\"speck2edevkit:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck-rescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
