{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sinabs.layers as sl\n",
    "import tqdm\n",
    "\n",
    "from tonic.datasets.nmnist import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5a0c1afb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./NMNIST\"\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 50\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_train_dataloader = DataLoader(snn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Module\n",
    "\n",
    "We need to define a `nn.Module` implementing the network we want the chip to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('device: ', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 10, 2, 1, bias=False) \n",
    "        self.iaf1 = sl.IAFSqueeze(batch_size=1)            \n",
    "        self.pool1 = sl.SumPool2d(3,3)                  \n",
    "        self.pool1a = sl.SumPool2d(4,4)                 \n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 10, 4, 1, bias=False)\n",
    "        self.iaf2 = sl.IAFSqueeze(batch_size=1)            \n",
    "\n",
    "        self.conv3 = nn.Conv2d(10, 1, 2, 1, bias=False) \n",
    "        self.iaf3 = sl.IAFSqueeze(batch_size=1)            \n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(49, 100, bias=False)       \n",
    "        self.iaf4 = sl.IAFSqueeze(batch_size=1)            \n",
    "        \n",
    "        self.fc2 = nn.Linear(100, 10, bias=False)       \n",
    "        self.iaf5 = sl.IAFSqueeze(batch_size=1)            \n",
    "\n",
    "\n",
    "    def detach_neuron_states(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if name != '':\n",
    "                if isinstance(layer, sl.StatefulLayer):\n",
    "                    for name, buffer in layer.named_buffers():\n",
    "                        buffer.detach_()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        con1_out = self.conv1(x)\n",
    "        iaf1_out = self.iaf1(con1_out)\n",
    "        pool1_out = self.pool1(iaf1_out)\n",
    "\n",
    "        conv2_out = self.conv2(pool1_out)\n",
    "        iaf2_out = self.iaf2(conv2_out)\n",
    "\n",
    "        conv3_out = self.conv3(iaf2_out)\n",
    "        iaf3_out = self.iaf3(conv3_out)\n",
    "\n",
    "        flat_out = self.flat(iaf3_out)\n",
    "        \n",
    "        fc1_out = self.fc1(flat_out)\n",
    "        iaf4_out = self.iaf4(fc1_out)\n",
    "        fc2_out = self.fc2(iaf4_out)\n",
    "        iaf5_out = self.iaf5(fc2_out)\n",
    "\n",
    "        return iaf5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = SNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(snn.parameters(), lr=0.001)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epochs, test_func, dataloader_test):\n",
    "    epochs_y = []\n",
    "    epochs_x = []\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        batches = []\n",
    "        batch_count = 0\n",
    "        train_p_bar = tqdm(snn_train_dataloader)\n",
    "\n",
    "        for X, y in train_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            X = X.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "            # forward\n",
    "            pred = model(X)\n",
    "\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            pred = pred.reshape(batch_size, n_time_steps, -1)\n",
    "\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            pred = pred.sum(dim = 1)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # detach the neuron states and activations from current computation graph(necessary)\n",
    "            model.detach_neuron_states()\n",
    "\n",
    "            train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "            batch_count += 1\n",
    "            losses.append(loss.item())\n",
    "            batches.append(batch_count)\n",
    "\n",
    "        epochs_y.append(losses)\n",
    "        epochs_x.append(batches)\n",
    "\n",
    "        acc = test_func(dataloader_test, model)\n",
    "        print(f'Epoch {e} accuracy: {acc}')\n",
    "\n",
    "    return epochs_x, epochs_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(dataloader)\n",
    "        for X, y in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            X = X.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "            # forward\n",
    "            output = model(X)\n",
    "\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(y.view_as(pred)))\n",
    "\n",
    "            test_p_bar.set_description(f\"Testing Model...\")\n",
    "    \n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    return correct_predictions.sum().item()/(len(correct_predictions))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(snn_train_dataloader, snn, loss_fn, optimizer, test, snn_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck-rescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
