{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetworkGraph\n",
    "from sinabs.layers import Merge, IAFSqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f66d5f875d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 2\n",
    "height = 34\n",
    "width = 34\n",
    "\n",
    "input_shape = (channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Module\n",
    "\n",
    "We need to define a `nn.Module` implementing the network we want the chip to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 10, 2, 1, bias=False) # node 0\n",
    "        self.iaf1 = IAFSqueeze(batch_size=1)            # node 1\n",
    "        self.pool1 = nn.AvgPool2d(3,3)                  # node 2\n",
    "        self.pool1a = nn.AvgPool2d(4,4)                 # node 3\n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 10, 4, 1, bias=False)# node 4\n",
    "        self.iaf2 = IAFSqueeze(batch_size=1)            # node 6\n",
    "        # self.pool2 = nn.AvgPool2d(3,3)                  # node 7\n",
    "\n",
    "        self.conv3 = nn.Conv2d(10, 1, 2, 1, bias=False) # node 8\n",
    "        self.iaf3 = IAFSqueeze(batch_size=1)            # node 9\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(49, 500, bias=False)       # node 10\n",
    "        self.iaf4 = IAFSqueeze(batch_size=1)            # node 11\n",
    "        \n",
    "        self.fc2 = nn.Linear(500, 10, bias=False)       # node 12\n",
    "        self.iaf5 = IAFSqueeze(batch_size=1)            # node 13\n",
    "\n",
    "        self.adder = Merge()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        con1_out = self.conv1(x)\n",
    "        iaf1_out = self.iaf1(con1_out)\n",
    "        pool1_out = self.pool1(iaf1_out)\n",
    "        pool1a_out = self.pool1a(iaf1_out)\n",
    "\n",
    "        conv2_out = self.conv2(pool1_out)\n",
    "        iaf2_out = self.iaf2(conv2_out)\n",
    "        # pool2_out = self.pool2(iaf2_out)\n",
    "\n",
    "        conv3_out = self.conv3(self.adder(pool1a_out, iaf2_out))\n",
    "        iaf3_out = self.iaf3(conv3_out)\n",
    "\n",
    "        flat_out = self.flat(iaf3_out)\n",
    "        \n",
    "        fc1_out = self.fc1(flat_out)\n",
    "        iaf4_out = self.iaf4(fc1_out)\n",
    "        fc2_out = self.fc2(iaf4_out)\n",
    "        iaf5_out = self.iaf5(fc2_out)\n",
    "\n",
    "        return iaf5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = SNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exemplifies how each of the layers in the SNN should be grouped together to form a `DynapcnnLayer` instance. Let's check the shapes of the tensors each of the (original) layers in the model inputs and outputs by feeding a fake input through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynapcnnLayer 0:                                ... [1, 2, 34, 34]\n",
      "                                                conv1: [1, 10, 33, 33]\n",
      "                                                iaf1: [1, 10, 33, 33]\n",
      "                                                pool1: [1, 10, 11, 11]\n",
      "                                                pool1a: [1, 10, 8, 8]\n",
      "\n",
      "DynapcnnLayer 1:                                ... [1, 10, 11, 11]\n",
      "                                                conv2: [1, 10, 8, 8]\n",
      "                                                iaf2: [1, 10, 8, 8]\n",
      "\n",
      "DynapcnnLayer 2:                                ... [1, 10, 8, 8] [ Merge(pool1a, iaf2_out) ]\n",
      "                                                conv3: [1, 1, 7, 7]\n",
      "                                                iaf3: [1, 1, 7, 7]\n",
      "\n",
      "DynapcnnLayer 3:                                ... [1, 49]\n",
      "                                                fc1: [1, 500]\n",
      "                                                iaf4: [1, 500]\n",
      "\n",
      "DynapcnnLayer 4:                                ... [1, 500]\n",
      "                                                fc2: [1, 10]\n",
      "                                                iaf5: [1, 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, *input_shape))\n",
    "\n",
    "print(f'DynapcnnLayer 0:                                ... {list(x.shape)}')\n",
    "con1_out = snn.conv1(x)\n",
    "print(f'                                                conv1: {list(con1_out.shape)}')\n",
    "iaf1_out = snn.iaf1(con1_out)\n",
    "print(f'                                                iaf1: {list(iaf1_out.shape)}')\n",
    "pool1_out = snn.pool1(iaf1_out)\n",
    "print(f'                                                pool1: {list(pool1_out.shape)}')\n",
    "pool1a_out = snn.pool1a(iaf1_out)\n",
    "print(f'                                                pool1a: {list(pool1a_out.shape)}\\n')\n",
    "\n",
    "print(f'DynapcnnLayer 1:                                ... {list(pool1_out.shape)}')\n",
    "conv2_out = snn.conv2(pool1_out)\n",
    "print(f'                                                conv2: {list(conv2_out.shape)}')\n",
    "iaf2_out = snn.iaf2(conv2_out)\n",
    "print(f'                                                iaf2: {list(iaf2_out.shape)}\\n')\n",
    "# pool2_out = snn.pool2(iaf2_out)\n",
    "# print(f'                                                pool2: {list(pool2_out.shape)}\\n')\n",
    "\n",
    "added = snn.adder(pool1a_out, iaf2_out)\n",
    "\n",
    "print(f'DynapcnnLayer 2:                                ... {list(added.shape)} [ Merge(pool1a, iaf2_out) ]')\n",
    "conv3_out = snn.conv3(added)\n",
    "print(f'                                                conv3: {list(conv3_out.shape)}')\n",
    "iaf3_out = snn.iaf3(conv3_out)\n",
    "print(f'                                                iaf3: {list(iaf3_out.shape)}\\n')\n",
    "\n",
    "flat_out = snn.flat(iaf3_out)\n",
    "\n",
    "print(f'DynapcnnLayer 3:                                ... {list(flat_out.shape)}')\n",
    "fc1_out = snn.fc1(flat_out)\n",
    "print(f'                                                fc1: {list(fc1_out.shape)}')\n",
    "iaf4_out = snn.iaf4(fc1_out)\n",
    "print(f'                                                iaf4: {list(iaf4_out.shape)}\\n')\n",
    "\n",
    "\n",
    "print(f'DynapcnnLayer 4:                                ... {list(iaf4_out.shape)}')\n",
    "fc2_out = snn.fc2(iaf4_out)\n",
    "print(f'                                                fc2: {list(fc2_out.shape)}')\n",
    "iaf5_out = snn.iaf5(fc2_out)\n",
    "print(f'                                                iaf5: {list(iaf5_out.shape)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynapcnnNetwork Class\n",
    "\n",
    "In the constructor of `DynapcnnNetworkGraph` the SNN passed as argument (defined as a `nn.Module`) will be parsed such that each layer is represented in a computational graph (using `nirtorch.extract_torch_graph`). \n",
    "\n",
    "The layers are the `nodes` of the graph, while their connectivity (how the outputs from a layer are sent to other layers) is represented as `edges`, represented in a `list` of `tuples`.\n",
    "\n",
    "Once the constructor finishes its initialization, the `hw_model.dynapcnn_layers` property is a dictionary where each entry represents the ID of a `DynapcnnLayer` instance (an `int` from `0` to `L`), with this entry containing a `DynapcnnLayer` instance where a subset of the layers in the original SNN has been incorporated into, the core such instance has been assigned to, and the list of `DynapcnnLayer` instances (their IDs) the layer targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_model = DynapcnnNetworkGraph(\n",
    "    snn,\n",
    "    discretize=True,\n",
    "    input_shape=input_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hw_model.to()` call will figure out into which core eac `DynapcnnLayer` instance will be assigned to. Once this assingment is made the instance itself is used to configure the `CNNLayerConfig` instance representing the core's configuration.\n",
    "\n",
    "If the cores' configuration is valid, each `DynapcnnLayer` instance and their respective destinations will be used to create a computational graph that encodes how the `forward` method of `hw_model.network` (a `nn.Module` using the `DynapcnnLayer` instances) propagates that through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DynapcnnNetworkGraph(\n",
       "  (network): DynapcnnNetworkModule()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_model.to(device=\"speck2edevkit:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers comprising our `hw_model` and their respective metadata can be inspected by calling `print` on a `DynapcnnNetworkGraph` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DynapcnnLayer 0 ----------------------------------------------------------\n",
      "> layer modules: \n",
      "(node 0): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 1): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1.), min_v_mem=Parameter containing:\n",
      "tensor(-32768.), batch_size=1, num_timesteps=-1)\n",
      "(node 2): SumPool2d(norm_type=1, kernel_size=(3, 3), stride=None, ceil_mode=False)\n",
      "(node 3): SumPool2d(norm_type=1, kernel_size=(4, 4), stride=None, ceil_mode=False)\n",
      "> layer destinations: [1, 2]\n",
      "> assigned core: 0\n",
      "\n",
      "---- DynapcnnLayer 1 ----------------------------------------------------------\n",
      "> layer modules: \n",
      "(node 4): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "(node 6): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1.), min_v_mem=Parameter containing:\n",
      "tensor(-32768.), batch_size=1, num_timesteps=-1)\n",
      "> layer destinations: [2]\n",
      "> assigned core: 1\n",
      "\n",
      "---- DynapcnnLayer 2 ----------------------------------------------------------\n",
      "> layer modules: \n",
      "(node 7): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 8): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1.), min_v_mem=Parameter containing:\n",
      "tensor(-32768.), batch_size=1, num_timesteps=-1)\n",
      "> layer destinations: [3]\n",
      "> assigned core: 2\n",
      "\n",
      "---- DynapcnnLayer 3 ----------------------------------------------------------\n",
      "> layer modules: \n",
      "(node 9): Conv2d(1, 500, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
      "(node 10): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1.), min_v_mem=Parameter containing:\n",
      "tensor(-32768.), batch_size=1, num_timesteps=-1)\n",
      "> layer destinations: [4]\n",
      "> assigned core: 3\n",
      "\n",
      "---- DynapcnnLayer 4 ----------------------------------------------------------\n",
      "> layer modules: \n",
      "(node 11): Conv2d(500, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "(node 12): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1.), min_v_mem=Parameter containing:\n",
      "tensor(-32768.), batch_size=1, num_timesteps=-1)\n",
      "> layer destinations: []\n",
      "> assigned core: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our DynapcnnNetwork\n",
    "\n",
    "Preparing the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://synsense.gitlab.io/sinabs-dynapcnn/getting_started/notebooks/nmnist_quick_start.html\n",
    "\n",
    "from tonic.datasets.nmnist import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from tqdm.notebook import tqdm\n",
    "    \n",
    "# download dataset\n",
    "root_dir = \"./NMNIST\"\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of data is: <class 'numpy.ndarray'>\n",
      "(4686,)\n",
      "time length of sample data is: 300760 micro seconds\n",
      "there are 4686 events in the sample data\n",
      "the label of the sample data is: 5\n"
     ]
    }
   ],
   "source": [
    "sample_data, label = NMNIST(save_to=root_dir, train=False)[0]\n",
    "\n",
    "print(f\"type of data is: {type(sample_data)}\")\n",
    "print(sample_data.shape)\n",
    "print(f\"time length of sample data is: {sample_data['t'][-1] - sample_data['t'][0]} micro seconds\")\n",
    "print(f\"there are {len(sample_data)} events in the sample data\")\n",
    "print(f\"the label of the sample data is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (100, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "\n",
    "# check the transformed data\n",
    "sample_data, label = snn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the training hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda:0')\n",
    "#     print('device: ', torch.cuda.get_device_name(0))\n",
    "# else:\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda:0\"\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_train_dataloader = DataLoader(snn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing our weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_model.network.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynapcnnNetworkModule()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_model.network.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(params=hw_model.network.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77410e6f9aab4c7a9515e31cec5c52de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_p_bar \u001b[38;5;241m=\u001b[39m tqdm(snn_train_dataloader)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m train_p_bar:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(snn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = hw_model.network(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        hw_model.network.detach_neuron_states()\n",
    "        \n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(snn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = hw_model.network(data)\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - BPTT Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - BPTT accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck-rescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
