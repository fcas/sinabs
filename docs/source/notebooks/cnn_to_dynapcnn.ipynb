{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a CNN model to a DYNAP-CNN DevKit\n",
    "\n",
    "This tutorial explains all steps necessary to convert a torch CNN model to a configuration of the DYNAP-CNN chip. We will first convert the network to a spiking neural network (SNN) and then to a `DynapcnnNetwork` â€“ a model that is compatible with the chip and simulates its behavior. Finally we port the model to a DYNAP-CNN DevKit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Before we start, we will import the libraries necessary to define a torch model, convert it to an SNN and to convert the SNN to a `DynapcnnNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Suppress warnings (This is only to keep the notebook pretty. You might want to comment the below two lines)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# - Import statements\n",
    "import torch\n",
    "import samna\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import io\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from sinabs.backend.dynapcnn.chip_factory import ChipFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN definition\n",
    "\n",
    "First we will define a sequential CNN model.\n",
    "\n",
    "*Note that although non-sequential models are supported by the hardware, this is not yet the case for this library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Define CNN model\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(20, 32, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(32, 128, 3, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 500, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 10, bias=False),\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "ann.load_state_dict(torch.load(\"../../../examples/mnist_params.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 5 parameter layers in the above defined model. You will see this come into play later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Spiking CNN\n",
    "\n",
    "We can use the `from_torch` method from SINABS to convert our CNN to a SNN. The returned object contains the original CNN as `analog_model` and the newly generated SNN as `spiking_model`. The `ReLU`s have been converted to `SpikingLayers`. In addition, we have also added a spiking layer at the end. This is for compatibitlity with the chip later on, since the chips can only produce spikes and not  activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (1): IAFSqueeze\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (4): IAFSqueeze\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (7): IAFSqueeze\n",
      "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=128, out_features=500, bias=False)\n",
      "  (11): IAFSqueeze\n",
      "  (12): Linear(in_features=500, out_features=10, bias=False)\n",
      "  (Spiking output): IAFSqueeze\n",
      ")\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "sinabs_model = from_model(ann, add_spiking_output=True, min_v_mem=-1)\n",
    "print(sinabs_model.spiking_model)\n",
    "\n",
    "print(sinabs_model.spiking_model[1].min_v_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DYNAP-CNN compatible network\n",
    "\n",
    "The next step is to convert the SNN to a `DynapcnnNetwork`. This way we can be sure that all functionalities of our network are supported by the hardware and we can simulate the expected hardware output for testing purposes. This object will also generate the configuration objects to set up the chip.\n",
    "\n",
    "We need to tell the chip the dimensions of the input data. This can be done either by specifying an `input_shape` argument in the constructor or including a SINABS `InputLayer` at the beginning of the model.\n",
    "\n",
    "The class will convert the parameters (weights, biases, and thresholds) to discrete values that are supported by DYNAP-CNN. For testing purposes this can be disabled by setting `discretize` to `False`.\n",
    "\n",
    "We can use the `dvs_input` flag to determine whether the chip should process data coming from the on-chip dynamic vision sensor (DVS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynapcnnNetwork(\n",
      "  (sequence): Sequential(\n",
      "    (0): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (1): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (2): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (3): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(128, 500, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze\n",
      "    )\n",
      "    (4): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(500, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# - Input dimensions\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# - DYNAP-CNN compatible network\n",
    "dynapcnn_net = DynapcnnNetwork(\n",
    "    sinabs_model.spiking_model,\n",
    "    input_shape=input_shape,\n",
    "    discretize=True,\n",
    "    dvs_input=False,\n",
    ")\n",
    "print(dynapcnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model consists of 5 `DynapcnnLayer` objects, each containing a convolutional, a spiking, and possibly a pooling layer. Because there were 5 parameter layers (as we pointed out earlier), each gets mapped into a separate layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Lets pass some data and see how this converted spiking model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset for spiking input data\n",
    "class MNIST_Dataset(datasets.MNIST):\n",
    "\n",
    "    def __init__(self, root, train = True, spiking=False, tWindow=100):\n",
    "        super().__init__(root, train=train, download=True)\n",
    "        self.spiking=spiking\n",
    "        self.tWindow = tWindow\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.spiking:\n",
    "            img = (np.random.rand(self.tWindow, 1, *img.size()) < img.numpy()/255.0).astype(float)\n",
    "            img = torch.from_numpy(img).float()\n",
    "        else:\n",
    "            # Convert image to tensor\n",
    "            img = torch.from_numpy(img.numpy()).float()\n",
    "            img.unsqueeze_(0)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Define dataloader\n",
    "tWindow = 200 # ms (or) time steps\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = MNIST_Dataset(\"./data\", train=False, spiking=True, tWindow=tWindow)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "525f3fd89912455c9a79226e25dc7641"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    # we will only use the first 500 samples to save some time\n",
    "    pbar = tqdm(test_dataset, total=500)\n",
    "    for data, label in pbar:\n",
    "        dynapcnn_net.reset_states()\n",
    "        out = dynapcnn_net(data)\n",
    "\n",
    "        # Calculate total number of spikes out\n",
    "        pred = out.squeeze().sum(0)\n",
    "        \n",
    "        # Check if the prediction matches the label\n",
    "        if pred.argmax() == label:\n",
    "            correct += 1\n",
    "        samples += 1\n",
    "        pbar.set_postfix(acc=100*correct/samples)\n",
    "        if samples >= 500:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([200, 1, 28, 28])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy of this model can now be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy of the  dynapcnn_net is: 97.8%'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Accuracy of the  dynapcnn_net is: {100*correct/samples}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting model to DYNAP-CNN DevKit\n",
    "\n",
    "Similar to porting a model to `cpu` or `gpu` in pytorch, the `DynapcnnNetwork` is a special class that supports porting a model to hardware based on DYNAP-CNN technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dynapcnndevkit:0'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply model to device such as dynapcnndevkit, speck2, speck2b\u001B[39;00m\n\u001B[1;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynapcnndevkit:0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mdynapcnn_net\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Work/git/sinabs-dynapcnn/venv/lib/python3.9/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:165\u001B[0m, in \u001B[0;36mDynapcnnNetwork.to\u001B[0;34m(self, device, chip_layers_ordering, monitor_layers, config_modifier)\u001B[0m\n\u001B[1;32m    157\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_config(\n\u001B[1;32m    158\u001B[0m     chip_layers_ordering\u001B[38;5;241m=\u001B[39mchip_layers_ordering,\n\u001B[1;32m    159\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m    160\u001B[0m     monitor_layers\u001B[38;5;241m=\u001B[39mmonitor_layers,\n\u001B[1;32m    161\u001B[0m     config_modifier\u001B[38;5;241m=\u001B[39mconfig_modifier,\n\u001B[1;32m    162\u001B[0m )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;66;03m# Apply configuration to device\u001B[39;00m\n\u001B[0;32m--> 165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamna_device \u001B[38;5;241m=\u001B[39m \u001B[43mopen_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamna_device\u001B[38;5;241m.\u001B[39mget_model()\u001B[38;5;241m.\u001B[39mapply_configuration(config)\n\u001B[1;32m    167\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Work/git/sinabs-dynapcnn/venv/lib/python3.9/site-packages/sinabs/backend/dynapcnn/io.py:202\u001B[0m, in \u001B[0;36mopen_device\u001B[0;34m(device_id)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03mOpen device function.\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;124;03m        Device handle received from samna.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    201\u001B[0m get_device_map()\n\u001B[0;32m--> 202\u001B[0m device_info \u001B[38;5;241m=\u001B[39m \u001B[43mdevice_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdevice_id\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    203\u001B[0m device_handle \u001B[38;5;241m=\u001B[39m samna\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mopen_device(device_info)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device_handle:  \u001B[38;5;66;03m# If the device was opened properly.\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'dynapcnndevkit:0'"
     ]
    }
   ],
   "source": [
    "# Apply model to device such as dynapcnndevkit, speck2, speck2b\n",
    "device = \"dynapcnndevkit:0\"\n",
    "dynapcnn_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can findout where the individual layers have been placed on the chip by looking at variable `chip_layers_ordering`. This information will come in handy when trying to send or receive events from the chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 5, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dynapcnn_net.chip_layers_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspecting memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model described above, we were able to find a sequence of layers on the chip which could be utilized. If we had a larger model that does not fit the chip, the `to` method call would have raised a `ValueError`. In that case, it would be handy to understand the model's memory requirements so that it can be modified to fit the chip. A quick overview of the memory requirements of the model can be found by calling the method `memory_summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'kernel': [1024.0, 20480.0, 65536.0, 65536.0, 8000.0],\n 'neuron': [20480.0, 2048.0, 512.0, 500.0, 10.0],\n 'bias': [0, 0, 0, 0, 0]}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynapcnn_net.memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this memory is not the same as the total number of kernel parameters. See the `memory_summary` documentation for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `memory_summary()` method returns the number of parameters/memory required for each layer of the  `DynapcnnNetwork` for kernel, neruons and bias. We see the corresponding values for the 5 layers of our current model above.\n",
    "\n",
    "You can compare these values to the `constrains` of the chip. For instance, the chip constraints for your device can be viewed from `DynapcnnConfigBuilder.get_constriants()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n LayerConstraints(kernel_memory=32768, neuron_memory=32768, bias_memory=1024),\n LayerConstraints(kernel_memory=32768, neuron_memory=32768, bias_memory=1024),\n LayerConstraints(kernel_memory=65536, neuron_memory=16384, bias_memory=1024),\n LayerConstraints(kernel_memory=65536, neuron_memory=16384, bias_memory=1024),\n LayerConstraints(kernel_memory=16384, neuron_memory=16384, bias_memory=1024),\n LayerConstraints(kernel_memory=16384, neuron_memory=16384, bias_memory=1024)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = ChipFactory(device).get_config_builder()\n",
    "builder.get_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a receiving spikes\n",
    "\n",
    "Last but not least, we want to be able to send and receive spikes from the chips.\n",
    "\n",
    "So first we start by generating some spike events. Unlike simulations where the spikes were presented as a `tensor` (a raster of spikes), for the chip, we will send a sequence of custom event objects. We can convert a spike raster `tensor` to a list of events using the utility funciton `ChipFactory.raster_to_events()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster, label = test_dataset[0]\n",
    "\n",
    "factory = ChipFactory(device)\n",
    "first_layer_idx = dynapcnn_net.chip_layers_ordering[0] \n",
    "events_in = factory.raster_to_events(raster, layer=first_layer_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these events can be sent to the chip. You can do this similarly to what you would do for a `pytorch` model on GPU or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DynapcnnNetwork' object has no attribute 'samna_output_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m events_out \u001B[38;5;241m=\u001B[39m \u001B[43mdynapcnn_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevents_in\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Work/git/sinabs-dynapcnn/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Work/git/sinabs-dynapcnn/venv/lib/python3.9/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:315\u001B[0m, in \u001B[0;36mDynapcnnNetwork.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    312\u001B[0m         \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    313\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m _parse_device_string(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m ChipFactory\u001B[38;5;241m.\u001B[39msupported_devices\n\u001B[1;32m    314\u001B[0m     ):\n\u001B[0;32m--> 315\u001B[0m         _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamna_output_buffer\u001B[49m\u001B[38;5;241m.\u001B[39mget_events()  \u001B[38;5;66;03m# Flush buffer\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;66;03m# NOTE: The code to start and stop time stamping is device specific\u001B[39;00m\n\u001B[1;32m    317\u001B[0m         reset_timestamps(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/Work/git/sinabs-dynapcnn/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1177\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1175\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1176\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1177\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1178\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DynapcnnNetwork' object has no attribute 'samna_output_buffer'"
     ]
    }
   ],
   "source": [
    "events_out = dynapcnn_net(events_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mevents_out\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'events_out' is not defined"
     ]
    }
   ],
   "source": [
    "print(events_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We receive spike objects from the hardware as output. We expect them to be from the last layer of the model (chip_layers_ordering[-1]) and for the data we sent in, we expect most spikes to be from the neuron index equal to label. Let's convert those to a tensor/raster format for easy slicing. Since the last layer has dimensions (10, 1, 1) for (feature, y, x), we are interested in the feature value of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m output \u001B[38;5;241m=\u001B[39m factory\u001B[38;5;241m.\u001B[39mevents_to_raster(\u001B[43mevents_out\u001B[49m)\n\u001B[1;32m      2\u001B[0m output\u001B[38;5;241m.\u001B[39msum([\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'events_out' is not defined"
     ]
    }
   ],
   "source": [
    "output = factory.events_to_raster(events_out)\n",
    "output.sum([0,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that neuron 7 produced most spikes. Lets see what the original label of our data was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Success. Our model on the chip identified the input data correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}