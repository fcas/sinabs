{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: From a CNN model to a DYNAP-CNN DevKit\n",
    "\n",
    "This tutorial explains all steps necessary to convert a torch CNN model to a configuration of the DYNAP-CNN chip. We will first convert the network to a spiking neural network (SNN) and then to a `DynapcnnCompatibleNetwork` â€“ a model that is compatible with the chip and simulates its behavior. Finally we port the model to a DYNAP-CNN DevKit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Before we start, we will import the libraries necessary to define a torch model, convert it to an SNN and to convert the SNN to a `DynapcnnCompatibleNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Suppress warnings (This is only to keep the notebook pretty. You might want to comment the below two lines)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# - Import statements\n",
    "import torch\n",
    "import samna\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import io\n",
    "from sinabs.backend.dynapcnn import DynapcnnCompatibleNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN definition\n",
    "\n",
    "First we will define a sequential CNN model.\n",
    "\n",
    "*Note that although non-sequential models are supported by the hardware, this is not yet the case for this library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Define CNN model\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(20, 32, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(32, 128, 3, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 500, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 10, bias=False),\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "ann.load_state_dict(torch.load(\"../../../examples/mnist_params.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 5 parameter layers in the above defined model. You will see this come into play later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Spiking CNN\n",
    "\n",
    "We can use the `from_torch` method from SINABS to convert our CNN to a SNN. The returned object contains the original CNN as `analog_model` and the newly generated SNN as `spiking_model`. The `ReLU`s have been converted to `SpikingLayers`. In addition, we have also added a spiking layer at the end. This is for compatibitlity with the chip later on, since the chips can only produce spikes and not  activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (1): SpikingLayer()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (4): SpikingLayer()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (7): SpikingLayer()\n",
      "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=128, out_features=500, bias=False)\n",
      "  (11): SpikingLayer()\n",
      "  (12): Linear(in_features=500, out_features=10, bias=False)\n",
      "  (Spiking output): SpikingLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sinabs_model = from_model(ann, add_spiking_output=True)\n",
    "print(sinabs_model.spiking_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DYNAP-CNN compatible network\n",
    "\n",
    "The next step is to convert the SNN to a `DynapcnnCompatibleNetwork`. This way we can be sure that all functionalities of our network are supported by the hardware and we can simulate the expected hardware output for testing purposes. This object will also generate the configuration objects to set up the chip.\n",
    "\n",
    "We need to tell the chip the dimensions of the input data. This can be done either by specifying an `input_shape` argument in the constructor or including a SINABS `InputLayer` at the beginning of the model.\n",
    "\n",
    "The class will convert the parameters (weights, biases, and thresholds) to discrete values that are supported by DYNAP-CNN. For testing purposes this can be disabled by setting `discretize` to `False`.\n",
    "\n",
    "We can use the `dvs_input` flag to determine whether the chip should process data coming from the on-chip dynamic vision sensor (DVS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynapcnnCompatibleNetwork(\n",
      "  (sequence): Sequential(\n",
      "    (0): DynapcnnLayer(\n",
      "      (_conv_layer): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (_spk_layer): SpikingLayer()\n",
      "      (_pool_layer): SumPool2d(norm_type=1, kernel_size=2, stride=2, ceil_mode=False)\n",
      "    )\n",
      "    (1): DynapcnnLayer(\n",
      "      (_conv_layer): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (_spk_layer): SpikingLayer()\n",
      "      (_pool_layer): SumPool2d(norm_type=1, kernel_size=2, stride=2, ceil_mode=False)\n",
      "    )\n",
      "    (2): DynapcnnLayer(\n",
      "      (_conv_layer): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (_spk_layer): SpikingLayer()\n",
      "      (_pool_layer): SumPool2d(norm_type=1, kernel_size=2, stride=2, ceil_mode=False)\n",
      "    )\n",
      "    (3): DynapcnnLayer(\n",
      "      (_conv_layer): Conv2d(128, 500, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_spk_layer): SpikingLayer()\n",
      "      (_pool_layer): SumPool2d(norm_type=1, kernel_size=1, stride=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): DynapcnnLayer(\n",
      "      (_conv_layer): Conv2d(500, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_spk_layer): SpikingLayer()\n",
      "      (_pool_layer): SumPool2d(norm_type=1, kernel_size=1, stride=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# - Input dimensions\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# - DYNAP-CNN compatible network\n",
    "dynapcnn_net = DynapcnnCompatibleNetwork(\n",
    "    sinabs_model.spiking_model,\n",
    "    input_shape=input_shape,\n",
    "    discretize=True,\n",
    "    dvs_input=False,\n",
    ")\n",
    "print(dynapcnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model consists of 5 `DynapcnnLayer` objects, each containing a convolutional, a spiking, and possibly a pooling layer. Because there were 5 parameter layers (as we pointed out earlier), each gets mapped into a separate layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Lets pass some data and see how this converted spiking model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset for spiking input data\n",
    "class MNIST_Dataset(datasets.MNIST):\n",
    "\n",
    "    def __init__(self, root, train = True, spiking=False, tWindow=100):\n",
    "        super().__init__(root, train=train, download=True)\n",
    "        self.spiking=spiking\n",
    "        self.tWindow = tWindow\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.spiking:\n",
    "            img = (np.random.rand(self.tWindow, 1, *img.size()) < img.numpy()/255.0).astype(float)\n",
    "            img = torch.from_numpy(img).float()\n",
    "        else:\n",
    "            # Convert image to tensor\n",
    "            img = torch.from_numpy(img.numpy()).float()\n",
    "            img.unsqueeze_(0)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader\n",
    "tWindow = 200 # ms (or) time steps\n",
    "\n",
    "# Define test dataset loader\n",
    "test_dataset = MNIST_Dataset(\"./data\", train=False, spiking=True, tWindow=tWindow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    pbar = tqdm(test_dataset)\n",
    "    for data, label in pbar:\n",
    "        out = dynapcnn_net(data)\n",
    "        \n",
    "        # Calculate total number of spikes out\n",
    "        pred = out.squeeze().sum(0)\n",
    "        \n",
    "        # Check if the prediction matches the label\n",
    "        if pred.argmax() == label:\n",
    "            correct += 1\n",
    "        samples += 1\n",
    "        pbar.set_postfix(acc=100*correct/samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy of this model can now be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Accuracy of the  dynapcnn_net is: {100*correct/samples}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting model to DYNAP-CNN DevKit\n",
    "\n",
    "Similar to porting a model to `cpu` or `gpu` in pytorch, the `DynapcnnCompatibleNetwork` is a special class that supports porting a model to hardware based on DYNAP-CNN technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to device\n",
    "dynapcnn_net.to(\n",
    "    device=\"dynapcnndevkit:0\",\n",
    "    chip_layers_ordering='auto',\n",
    "    monitor_layers=[8],\n",
    "    config_modifier=config_modifier,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the memory required by this model when mapped onto the chip.\n",
    "\n",
    "> Note that this memory is not the same as the total number of kernel parameters. See the `memory_summary` documentation for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynapcnn_net.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Run network on random data\n",
    "input_data = rand((1, *input_shape)) * 1000\n",
    "output_data = dynapcnn_net(input_data)\n",
    "\n",
    "# - Model is quantized, so the output will have an integer value.\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DYNAP-CNN configuration\n",
    "\n",
    "We can now extract a dynapcnn configuration object from the `DynapcnnCompatibleNetwork`, which can then be used to configure the hardware. In order to map layers of the sequential model to specific layers on the chip we can provide a list of layer indices in the order of the data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dynapcnn_net.make_config(dynapcnn_layers_ordering=[4, 2, 1, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and upload to DYNAP-CNN\n",
    "\n",
    "Finally, we only need to upload the configuration to the hardware. Before that however, it should be made sure that the way the layers are arranged is compatible.\n",
    "\n",
    "The functionalities for validation and uploading will soon be added and explained here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
