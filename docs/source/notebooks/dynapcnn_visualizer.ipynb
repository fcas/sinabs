{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing DynapCNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the development kits that SynSense provides are mainly used for benchmarking purposes. However, the through our backend library `samna` we do support processing of events and interpretation of the output in a streaming fashion. `samna` API however is mainly designed and developed for low-level communication with the chips. This sometimes makes it complicated and hard to work with. `samna` also has a package called `samnagui` with which we can do some visualization. Internally, we use it for testing of our models real-time in real-life conditions. \n",
    "In order to give users an easy access without having to deal with a lot of boilerplate code and burdensome logic, we implemented a visualizer class.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available plots in `samnagui`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 different plots in `samnagui` that are available for use.\n",
    "- <b>Activity Plot:</b> A plot that visualizes the events produced by the on-chip sensor.\n",
    "- <b>Line Plot:</b> A plot that allows visualization of the events produced by the model running on the chip. Additionally this is used when we display power consumption measurements.\n",
    "- <b>Image Plot:</b> A plot that allows you to display an image. Internally we have used this plot to display an image denoting a predicted output class. <br>\n",
    "Documentation is available under the following link: [samna-viz-imgui documentation](`https://synsense-sys-int.gitlab.io/samna/reference/viz/imgui/index.html#samna-viz-imgui`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful `samna` nodes for visualizing and Just-In-Time (JIT) compiled nodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "`samna` also comes with implementation of several nodes that are useful for communicating between a `samnagui` visualizer and the chip. <br>\n",
    "The ones that are related to DVS events from sensor. With methods:\n",
    "- `DvsEventDecimate`: Eliminating `L` out of `M` events. <br>\n",
    "<t>`set_decimation_fraction(M: int, L: int)`\n",
    "- `DvsEventRescale`: Rescale events. `x / width` and `y / height`. <br>\n",
    "<t>`set_rescaling_coefficients(width_coeff: int, height_coeff: int)`\n",
    "- `DvsToVizConverter`: Converts events from sensor to visualization events. \n",
    "<br>The ones that are connected to the chip output spikes:\n",
    "- `SpikeCollectionNode`: Picks output spikes at intervals (ms) and makes events that can be used out of them. <br>\n",
    "<t>`set_interval_milli_sec(interval: int)`\n",
    "- `SpikeCountNode`: Counts how many events from each output received among `feature_count` events and outputs a visualizer event. <br>\n",
    "<t>`set_feature_count(feature_count: int)`\n",
    "- `MajorityReadoutNode`: Among the events produced by `SpikeCollectionNode` selects the most active output channel. Used alongside the `ImagePlot`.\n",
    "\n",
    "These nodes are available for different chips. Further documentation can be found in the following link: [samna-available-filters](https://synsense-sys-int.gitlab.io/samna/filters.html) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-In-Time compiled nodes\n",
    "The nodes mentioned above are also available for any board under `samna.graph.Jit{nameOfNode}`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynapcnn Visualizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DynapcnnVisualizer` class uses the nodes and plots mentioned above in order to use it and provides a good and intuitive interface. An example can be found below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_folder_path = str( os.path.abspath(__file__) )\n",
    "file_tokens = current_folder_path.split(\"/\")[:-3]\n",
    "params_path = os.path.join( os.path.join(*file_tokens), \"examples/dvs_gesture_params.pt\") \n",
    "icons_folder_path = os.path.join( os.path.join(*file_tokens), \"examples/icons/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(2, 16, kernel_size=2, stride=2, bias=False),\n",
    "    nn.ReLU(),\n",
    "    # core 1\n",
    "    nn.Conv2d(16, 16, kernel_size=3, padding=1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # core 2\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # core 7\n",
    "    nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # core 4\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # core 5\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # core 6\n",
    "    nn.Dropout2d(0.5),\n",
    "    nn.Conv2d(64, 256, kernel_size=2, bias=False),\n",
    "    nn.ReLU(),\n",
    "    # core 3\n",
    "    nn.Dropout2d(0.5),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256, 128, bias=False),\n",
    "    nn.ReLU(),\n",
    "    # core 8\n",
    "    nn.Linear(128, 11, bias=False),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights from the example folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_result = ann.load_state_dict(torch.load(\"dvs_gesture_params.pt\"), strict=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to an `SNN` and a `DynapcnnNetwork` using `sinabs` and `sinabs-dynapcnn`. Important!<br>\n",
    "- `dvs_input`=`True` has to be set so that the model can receive input from the on-board sensor or an external DVS sensor.\n",
    "- `discretize`=`True` has to be set so that the model can be ported to the chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinabs_model = from_model(ann, add_spiking_output=True, batch_size=1)\n",
    "\n",
    "input_shape = (2, 128, 128)\n",
    "hardware_compatible_model = DynapcnnNetwork(\n",
    "    sinabs_model.spiking_model.cpu(),\n",
    "    dvs_input=True,\n",
    "    discretize=True,\n",
    "    input_shape=input_shape\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Port the model on the chip. Important! <br>\n",
    "- `monitor_layers` = `[\"dvs\", -1]` Other layers can also be monitored for other purposes, but `monitor_layers` should contain at least `dvs` and `-1` so that the visualizer has access to the `dvs` layer and `-1` for the output layer of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware_compatible_model.to(\n",
    "    device=\"speck2b\", # speck2edevkit\n",
    "    monitor_layers=[\"dvs\", -1],  # Last layer\n",
    "    chip_layers_ordering=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the class outputs as images, we need to get the images. The images should be passed in the same order as the output layer of the network. Important! <br>\n",
    "- If you want to visualize power measurements during streaming inference, set `add_power_monitor_plot`=`True`.\n",
    "- If you want to visualize readout images as class predictions during streaming you need to pass `add_readout_plot`=`True`.\n",
    "- In order to show a prediction for each `N` milliseconds, set the parameter `spike_collection_interval`=`N`.\n",
    "- In order to show the images, the paths of these images should be passed to `readout_images` parameter.\n",
    "- In order to show a prediction only if there are more than a `threshold` number of events from that output, set the `readout_prediction_threshold`=`threshold`.\n",
    "- In order to default to a certain class (in `DVSGesture` example, we use the last `other` class) set `readout_default_return_value`=`class_idx` (int).\n",
    "- In order to limit the prediction between some thresholds (i.e. it is meaningless to make a prediction with too low and too high values) set `readout_default_threshold_low` and `readout_default_threshold_high` parameters.\n",
    "- In all the chips except for `DynapcnnDevkit` you can monitor the power consumption in 5 different rows. These rows are `io`, `logic`, `ram`, `pixel-digital` and `pixel-analog`.\n",
    "    - `io`: Power consumption of IO unit on the chip.\n",
    "    - `logic`: Power consumption caused by operations in the chip and communication with spikes.\n",
    "    - `ram`: Power consumption of `memory` used to store `states` and `weights`.\n",
    "    - `pixel-digital`: Power consumption of the digital part (i.e. communication via AER) of pixels.\n",
    "    - `pixel-analog`: Power consumtpion of the analog part. (i.e. pixels.\n",
    "    ). \n",
    "- `DynapcnnDevkit` does not come with an on-board Dvs sensor, thus does not support the last two. If you want only the first 3 columns, pass `power_monitor_number_of_items`=`3`. If you want columns pass `5`.\n",
    "- If you follow the naming conventions for readout images (i.e. `{labelidx}_{labelname}.png`) the feature names are going to be parsed from the image names. However, you can also pass labels manually using `feature_names`=`[\"class01\", \"class02\", ...]`. That can be used also while labeling `SpikeCountPlot` legend, when `ReadoutLayer` is not preferred.\n",
    "- The last layer feature count is going to be automatically extracted from the model. However you can pass it manually `feature_count`=`count`. This can be useful for plotting purposes, when there are other classes that you did not take into account in the model you trained.\n",
    "- `extra_arguments`: You can pass the function names and variables for the individual plots for `spike_count`, `readout` and `power_measurement` plots. Available function names and argument types can be found in the following link: [here](https://synsense-sys-int.gitlab.io/samna/reference/viz/imgui/index.html#samna-viz-imgui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "visualizer = DynapcnnVisualizer(\n",
    "    dvs_shape=(128, 128),\n",
    "    add_power_monitor_plot=True,\n",
    "    add_readout_plot=True,\n",
    "    spike_collection_interval=500,\n",
    "    readout_images=sorted([os.path.join(icons_folder_path, f) for f in os.listdir(icons_folder_path)])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GitProjects-vURBfxcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7ba635a70a009fca501873764dd17758f210512767a21f7a85ff419cdce7c46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
